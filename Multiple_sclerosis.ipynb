{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiple_sclerosis.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"iXEF04sFLd1x","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukd9xzX0Liwy","colab_type":"code","outputId":"cc2fcaf0-7719-49cb-a3b7-b4d9839d1d9d","colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Jan  7 12:32:05 2019\n","\n","@author: Krishna\n","\"\"\"\n","# Importing neccessary packages\n","import h5py\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import nibabel as nib\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","# Setting the path\n","Path='drive/My Drive/Pre-processed'\n","p=os.listdir(Path)\n","\n","X_Dp=[]          \n","X_Gado=[]\n","X_Flair=[]      \n","X_T1=[]\n","X_T2=[]\n","Y_Man1=[]\n","\n","X=[]\n","Y1=[]\n","\n","# Loading the MRI Scans \n","for i in p[0:14] :\n","    \n","    q=os.listdir(os.path.join(Path,i))  \n","    \n","    x=nib.load(os.path.join(Path,i,q[0]))         \n","    f_Dp=x.get_fdata()\n","    f_Dp=np.asarray(f_Dp,'float32')\n","    for j in range(f_Dp.shape[2]):\n","        slice_Dp=cv.resize(f_Dp[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        if(np.sum(slice_Dp)!=0):\n","          #  slice_Dp=slice_Dp/(np.max(slice_Dp)+0.00001)\n","            slice_Dp=(slice_Dp-np.mean(slice_Dp)+0.00001)/(np.std(slice_Dp)+0.00001)\n","        X_Dp.append(slice_Dp)\n","    \n","      \n","    \n","    x=nib.load(os.path.join(Path,i,q[1]))\n","    f_Flair=x.get_fdata()\n","    f_Flair=np.asarray(f_Flair,'float32')\n","    for j in range(f_Flair.shape[2]):\n","        slice_Flair=cv.resize(f_Flair[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        if(np.sum(slice_Flair)!=0):\n","           # slice_Flair=slice_Flair/(np.max(slice_Flair)+0.00001)\n","            slice_Flair=(slice_Flair-np.mean(slice_Flair)+0.00001)/(np.std(slice_Flair)+0.00001)\n","        X_Flair.append(slice_Flair)\n","    \n","    \n","    x=nib.load(os.path.join(Path,i,q[2]))\n","    f_Gado=x.get_fdata()\n","    f_Gado=np.asarray(f_Gado,'float32')\n","    for j in range(f_Gado.shape[2]):\n","        slice_Gado=cv.resize(f_Gado[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        if(np.sum(slice_Gado)!=0):\n","           # slice_Gado=slice_Gado/(np.max(slice_Gado)+0.00001)\n","            slice_Gado=(slice_Gado-np.mean(slice_Gado)+0.00001)/(np.std(slice_Gado)+0.00001)\n","        X_Gado.append(slice_Gado)\n","    \n","    \n","    x=nib.load(os.path.join(Path,i,q[3]))\n","    f_Man1=x.get_fdata()\n","    f_Man1=np.asarray(f_Man1,'float32')\n","    for j in range(f_Man1.shape[2]):\n","        slice_Man1=cv.resize(f_Man1[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        slice_Man1=np.array(slice_Man1)\n","        slice_Man1[slice_Man1 > 0] = 1.0\n","        Y_Man1.append(slice_Man1)\n","    \n","    x=nib.load(os.path.join(Path,i,q[10]))\n","    f_T1=x.get_fdata()\n","    f_T1=np.asarray(f_T1,'float32')\n","    for j in range(f_T1.shape[2]):\n","        slice_T1=cv.resize(f_T1[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        if(np.sum(slice_T1)!=0):\n","        #    slice_T1=slice_T1/(np.max(slice_T1)+0.00001)\n","            slice_T1=(slice_T1-np.mean(slice_T1)+0.00001)/(np.std(slice_T1)+0.00001)\n","        X_T1.append(slice_T1)\n","    \n","    \n","    x=nib.load(os.path.join(Path,i,q[11]))\n","    f_T2=x.get_fdata()\n","    f_T2=np.asarray(f_T2,'float32')\n","    for j in range(f_T2.shape[2]):\n","        slice_T2=cv.resize(f_T2[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","        if(np.sum(slice_T2)!=0):\n","         #   slice_T2=slice_T2/(np.max(slice_T2)+0.00001)\n","            slice_T2=(slice_T2-np.mean(slice_T2)+0.00001)/(np.std(slice_T2)+0.00001)\n","        X_T2.append(slice_T2)\n","\n","for i in range(len(X_Dp)):\n","    X_Dp[i]=X_Dp[i].T\n","    X_Flair[i]=X_Flair[i].T\n","    X_Gado[i]=X_Gado[i].T\n","    X_T1[i]=X_T1[i].T\n","    X_T2[i]=X_T2[i].T\n","    Y_Man1[i]=Y_Man1[i].T\n","\n","\n","for i in range(len(X_Dp)):\n","    slice_Dp=X_Dp[i]\n","    slice_Dp=slice_Dp[:,:,np.newaxis]\n","    \n","    slice_Flair=X_Flair[i]\n","    slice_Flair=slice_Flair[:,:,np.newaxis]\n","    \n","    slice_Gado=X_Gado[i]\n","    slice_Gado=slice_Gado[:,:,np.newaxis]\n","    \n","    slice_T1=X_T1[i]\n","    slice_T1=slice_T1[:,:,np.newaxis]\n","    \n","    slice_T2=X_T2[i]\n","    slice_T2=slice_T2[:,:,np.newaxis]\n","    \n","    final_slice=np.concatenate((slice_Dp,slice_Flair,slice_Gado,slice_T1,slice_T2),axis=-1)\n","    \n","    if(np.sum(final_slice)!=0):\n","        X.append(final_slice)\n","        Y1.append(Y_Man1[i])\n","\n","        \n","        \n","X=np.array(X,dtype='float32')\n","Y1=np.array(Y1,dtype='float32')\n","\n","\n","Y1=Y1[:,:,:,np.newaxis]\n","\n","\n","#np.save(\"X\",X)\n","#np.save(\"Y\",Y1)\n","#\n","#X = np.load(\"G:/Multiple Scelerosis/load_data/X.npy\")\n","#Y = np.load(\"G:/Multiple Scelerosis/load_data/Y.npy\")\n","\n","with h5py.File('preprocessed_axial_data.h5', 'w') as hf:\n","\n","    hf.create_dataset('input_X', data=X[:])\n","    hf.create_dataset('Manual_1', data=Y1[:])\n","\n","#    \n","with h5py.File('inputs_no_null.h5', 'w') as hf:\n","\n","    hf.create_dataset('Dp', data=X_Dp[:])\n","    hf.create_dataset('Flair', data=X_Flair[:])\n","    hf.create_dataset('Gado', data=X_Gado[:])\n","    hf.create_dataset('T1', data=X_T1[:]) \n","    hf.create_dataset('T2', data=X_T2[:])\n","  #  hf.create_dataset('Y_Dp', data=Y_Dp[:])\n","   # hf.create_dataset('Y_Flair', data=Y_Flair[:])\n","   # hf.create_dataset('Y_Gado', data=Y_Gado[:])\n","#    hf.create_dataset('Y_T1', data=Y_T1[:]) \n"," #   hf.create_dataset('Y_T2', data=Y_T2[:])\n","    \n","        \n","\n","\n","    \n","\n","    "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"YKVZbl_EMWHk","colab_type":"code","outputId":"24c06f13-138a-4bc7-fda4-bd088dc1b5e4","executionInfo":{"status":"ok","timestamp":1582713639437,"user_tz":-330,"elapsed":26681,"user":{"displayName":"ABHIJIT ROY","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCRB_1h72fuTtW9GdzmfQp5np-aY61anpygp5Nt=s64","userId":"16674069840608108138"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PTdGxoJeZm97","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Dec 13 18:56:04 2019\n","\n","@author: Krishna Chandra\n","\"\"\"\n","import tensorflow as tf\n","import keras\n","from keras.models import Model, load_model\n","from keras.layers import Input ,BatchNormalization , Activation \n","from keras.layers.convolutional import Conv2D, UpSampling2D\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers.merge import concatenate\n","\n","\n","def Convolution(input_tensor,filters):\n","    \n","    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1))(input_tensor)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x) \n","    return x\n","\n","def model(input_shape):\n","    \n","    inputs = Input((input_shape))\n","    \n","    conv_1 = Convolution(inputs,32)\n","    maxp_1 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_1)\n","    \n","    conv_2 = Convolution(maxp_1,64)\n","    maxp_2 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_2)\n","    \n","    conv_3 = Convolution(maxp_2,128)\n","    maxp_3 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_3)\n","    \n","    conv_4 = Convolution(maxp_3,256)\n","    maxp_4 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_4)\n","    \n","    conv_5 = Convolution(maxp_4,512)\n","    upsample_6 = UpSampling2D((2, 2)) (conv_5)\n","    \n","    conv_6 = Convolution(upsample_6,256)\n","    upsample_7 = UpSampling2D((2, 2)) (conv_6)\n","    \n","    upsample_7 = concatenate([upsample_7, conv_3])\n","    \n","    conv_7 = Convolution(upsample_7,128)\n","    upsample_8 = UpSampling2D((2, 2)) (conv_7)\n","    \n","    conv_8 = Convolution(upsample_8,64)\n","    upsample_9 = UpSampling2D((2, 2)) (conv_8)\n","    \n","    upsample_9 = concatenate([upsample_9, conv_1])\n","    \n","    conv_9 = Convolution(upsample_9,32)\n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (conv_9)\n","    \n","    model = Model(inputs=[inputs], outputs=[outputs]) \n","    \n","    return model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U48-R7igZ8ZS","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Jan  7 12:32:05 2019\n","\n","@author: Krishna\n","\"\"\"\n","# Importing neccessary packages\n","import h5py\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import nibabel as nib\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","def process():\n","# Setting the path\n","    Path='drive/My Drive/Pre-processed'\n","    p=os.listdir(Path)\n","\n","    X_Dp=[]          \n","    X_Gado=[]\n","    X_Flair=[]      \n","    X_T1=[]\n","    X_T2=[]\n","    Y_Man1=[]\n","\n","    X=[]\n","    Y1=[]\n","\n","# Loading the MRI Scans \n","    for i in p[0:14] :\n","    \n","        q=os.listdir(os.path.join(Path,i))  \n","    \n","        x=nib.load(os.path.join(Path,i,q[0]))         \n","        f_Dp=x.get_fdata()\n","        f_Dp=np.asarray(f_Dp,'float32')\n","        for j in range(f_Dp.shape[2]):\n","            slice_Dp=cv.resize(f_Dp[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            if(np.sum(slice_Dp)!=0):\n","          #  slice_Dp=slice_Dp/(np.max(slice_Dp)+0.00001)\n","                slice_Dp=(slice_Dp-np.mean(slice_Dp)+0.00001)/(np.std(slice_Dp)+0.00001)\n","            X_Dp.append(slice_Dp)\n","    \n","      \n","    \n","        x=nib.load(os.path.join(Path,i,q[1]))\n","        f_Flair=x.get_fdata()\n","        f_Flair=np.asarray(f_Flair,'float32')\n","        for j in range(f_Flair.shape[2]):\n","            slice_Flair=cv.resize(f_Flair[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            if(np.sum(slice_Flair)!=0):\n","           # slice_Flair=slice_Flair/(np.max(slice_Flair)+0.00001)\n","                slice_Flair=(slice_Flair-np.mean(slice_Flair)+0.00001)/(np.std(slice_Flair)+0.00001)\n","            X_Flair.append(slice_Flair)\n","    \n","    \n","        x=nib.load(os.path.join(Path,i,q[2]))\n","        f_Gado=x.get_fdata()\n","        f_Gado=np.asarray(f_Gado,'float32')\n","        for j in range(f_Gado.shape[2]):\n","            slice_Gado=cv.resize(f_Gado[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            if(np.sum(slice_Gado)!=0):\n","           # slice_Gado=slice_Gado/(np.max(slice_Gado)+0.00001)\n","                slice_Gado=(slice_Gado-np.mean(slice_Gado)+0.00001)/(np.std(slice_Gado)+0.00001)\n","            X_Gado.append(slice_Gado)\n","    \n","    \n","        x=nib.load(os.path.join(Path,i,q[3]))\n","        f_Man1=x.get_fdata()\n","        f_Man1=np.asarray(f_Man1,'float32')\n","        for j in range(f_Man1.shape[2]):\n","            slice_Man1=cv.resize(f_Man1[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            slice_Man1=np.array(slice_Man1)\n","            slice_Man1[slice_Man1 > 0] = 1.0\n","            Y_Man1.append(slice_Man1)\n","    \n","        x=nib.load(os.path.join(Path,i,q[10]))\n","        f_T1=x.get_fdata()\n","        f_T1=np.asarray(f_T1,'float32')\n","        for j in range(f_T1.shape[2]):\n","            slice_T1=cv.resize(f_T1[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            if(np.sum(slice_T1)!=0):\n","        #    slice_T1=slice_T1/(np.max(slice_T1)+0.00001)\n","                slice_T1=(slice_T1-np.mean(slice_T1)+0.00001)/(np.std(slice_T1)+0.00001)\n","            X_T1.append(slice_T1)\n","    \n","    \n","        x=nib.load(os.path.join(Path,i,q[11]))\n","        f_T2=x.get_fdata()\n","        f_T2=np.asarray(f_T2,'float32')\n","        for j in range(f_T2.shape[2]):\n","            slice_T2=cv.resize(f_T2[:,:,j],(256,256),interpolation=cv.INTER_AREA)\n","            if(np.sum(slice_T2)!=0):\n","         #   slice_T2=slice_T2/(np.max(slice_T2)+0.00001)\n","                slice_T2=(slice_T2-np.mean(slice_T2)+0.00001)/(np.std(slice_T2)+0.00001)\n","            X_T2.append(slice_T2)\n","\n","    for i in range(len(X_Dp)):\n","        X_Dp[i]=X_Dp[i].T\n","        X_Flair[i]=X_Flair[i].T\n","        X_Gado[i]=X_Gado[i].T\n","        X_T1[i]=X_T1[i].T\n","        X_T2[i]=X_T2[i].T\n","        Y_Man1[i]=Y_Man1[i].T\n","\n","\n","    for i in range(len(X_Dp)):\n","        slice_Dp=X_Dp[i]\n","        slice_Dp=slice_Dp[:,:,np.newaxis]\n","    \n","        slice_Flair=X_Flair[i]\n","        slice_Flair=slice_Flair[:,:,np.newaxis]\n","    \n","        slice_Gado=X_Gado[i]\n","        slice_Gado=slice_Gado[:,:,np.newaxis]\n","    \n","        slice_T1=X_T1[i]\n","        slice_T1=slice_T1[:,:,np.newaxis]\n","    \n","        slice_T2=X_T2[i]\n","        slice_T2=slice_T2[:,:,np.newaxis]\n","    \n","        final_slice=np.concatenate((slice_Dp,slice_Flair,slice_Gado,slice_T1,slice_T2),axis=-1)\n","    \n","        if(np.sum(final_slice)!=0):\n","            X.append(final_slice)\n","            Y1.append(Y_Man1[i])\n","\n","        \n","        \n","    X=np.array(X,dtype='float32')\n","    Y1=np.array(Y1,dtype='float32')\n","\n","\n","    Y1=Y1[:,:,:,np.newaxis]\n","\n","\n","#np.save(\"X\",X)\n","#np.save(\"Y\",Y1)\n","#\n","#X = np.load(\"G:/Multiple Scelerosis/load_data/X.npy\")\n","#Y = np.load(\"G:/Multiple Scelerosis/load_data/Y.npy\")\n","\n","    with h5py.File('preprocessed_axial_data.h5', 'w') as hf:\n","\n","        hf.create_dataset('input_X', data=X[:])\n","        hf.create_dataset('Manual_1', data=Y1[:])\n","\n","#    \n","    with h5py.File('inputs_no_null.h5', 'w') as hf:\n","\n","        hf.create_dataset('Dp', data=Dp[:])\n","        hf.create_dataset('Flair', data=Flair[:])\n","        hf.create_dataset('Gado', data=Gado[:])\n","        hf.create_dataset('T1', data=T1[:]) \n","        hf.create_dataset('T2', data=T2[:])\n","     #   hf.create_dataset('Y_Dp', data=Y_Dp[:])\n","      #  hf.create_dataset('Y_Flair', data=Y_Flair[:])\n","       # hf.create_dataset('Y_Gado', data=Y_Gado[:])\n","       # hf.create_dataset('Y_T1', data=Y_T1[:]) \n","       # hf.create_dataset('Y_T2', data=Y_T2[:])\n","    \n","        \n","\n","\n","    \n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p-XXWG4auF_","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Dec 13 18:52:08 2019\n","\n","@author: Krishna Chandra\n","\"\"\"\n","\n","from keras import backend as K\n","import numpy as np\n","import tensorflow as tf\n","\n","# Computing Dice_Coefficient\n","def dice_coef(y_true, y_pred, smooth=1.0):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","\n","# Computing Precision \n","def precision(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","# Computing Sensitivity      \n","def sensitivity(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    return true_positives / (possible_positives + K.epsilon())\n","\n","# Computing Specificity\n","def specificity(y_true, y_pred):\n","    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n","    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n","    return true_negatives / (possible_negatives + K.epsilon())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0IpaR9ubA-o","colab_type":"code","outputId":"ddd457d4-c7e3-4aa9-ed5f-c2f13c8ce5f1","executionInfo":{"status":"error","timestamp":1582469444723,"user_tz":-330,"elapsed":14351,"user":{"displayName":"Abhijit Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCgTU57NEBdR4E81rvwtnKBuSqrCafb2ErnNH4T1g=s64","userId":"08205692553559323056"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Dec 13 20:18:06 2019\n","\n","@author: Krishna Chandra\n","\"\"\"\n","\n","\n","\n","import numpy as np\n","#import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","#import keras\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input ,BatchNormalization , Activation \n","from tensorflow.keras.layers import Conv2D, UpSampling2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import concatenate\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras import optimizers \n","from sklearn.model_selection import train_test_split\n","\n","\n","#import dataPrepare as process\n","#import Modified_UNet \n","#import plots\n","#import Metrics\n","\n","# Setting the path\n","Path='drive/My Drive/Pre-processed'\n","\n","\n","\n","# Loading all the 5 different modalities of each MRI Scan of all 15 different patients and 1st rater Manual Segmentation\n","'''X_Dp      =   modality(Path,0)\n","X_Flair   =   modality(Path,1)\n","X_Gado    =   modality(Path,2)\n","X_T1      =   modality(Path,10)\n","X_T2      =   modality(Path,11)\n","Y_Manual  =   modality(Path,3)\n","\n","# Removing the null samples and concatenating the 5 modalities along the 3rd dimension\n","X, Y = remove_null_samples(X_Dp, X_Flair, X_Gado, X_T1, X_T2, Y_Manual)\n","'''\n","\n","# Splitting the Whole data into Training and Testing data\n","X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=32)\n","\n","# Loding the modified U-net \n","model = model(input_shape = (256,256,5))\n","model.summary()\n","\n","checkpointer = ModelCheckpoint('Modified_UNet.h5', verbose=1)\n","callback_list=[checkpointer]\n","\n","# Compiling the model\n","Adam=optimizers.adam(lr=0.001)\n","model.compile(optimizer=Adam, loss='binary_crossentropy', metrics=['accuracy',dice_coef,precision,sensitivity,specificity])\n","# Fitting the model over the data\n","history = model.fit(X_train,Y_train,batch_size=32,epochs=60,validation_split=0.20,verbose=1,initial_epoch=0,callbacks=callback_list)\n","\n","# Saving the model\n","model.save('Modified_UNet.h5')\n","history.history\n","\n","# Evaluating the model on the training and testing data \n","model.evaluate(x=X_train, y=Y_train, batch_size=32 , verbose=1, sample_weight=None, steps=None)\n","model.evaluate(x=X_test, y=Y_test, batch_size=32, verbose=1, sample_weight=None, steps=None)\n","\n","# Plotting the Graphs of Accuracy, Dice_coefficient, Loss at each epoch on Training and Testing data\n","#plots.Accuarcy_Graph(history)\n","#plots.Dice_coefficient_Graph(history)\n","#plots.Loss_Graph(history)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-84ab9cbe36ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m '''\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Splitting the Whole data into Training and Testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Loding the modified U-net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"]}]}]}